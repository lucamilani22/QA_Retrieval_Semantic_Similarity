{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 2\n",
    "**Distributed Static Text Representation**. Choose a static distributed representation method we have seen in class, such as Word2vec, Doc2Vec, or pretrained embeddings like FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy / Google news Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these versions of the libraries in case of conflicts between them\n",
    "# Terminal\n",
    "# pip install pandas==2.2.3 numpy==1.26.4 scikit-learn==1.6.1 nltk==3.9.1 spacy==3.8.4 gensim==4.3.3\n",
    "# Jupyter Notebook\n",
    "# %pip install pandas==2.2.3 numpy==1.26.4 scikit-learn==1.6.1 nltk==3.9.1 spacy==3.8.4 gensim==4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucamilani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucamilani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lucamilani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU on Test Split (Ensemble spaCy + FastText): 0.085622132385527\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. Loading Data\n",
    "train_prompts = pd.read_csv(\"train_prompts.csv\")\n",
    "train_responses = pd.read_csv(\"train_responses.csv\")\n",
    "dev_prompts = pd.read_csv(\"dev_prompts.csv\")\n",
    "dev_responses = pd.read_csv(\"dev_responses.csv\")\n",
    "test_prompts = pd.read_csv(\"test_prompts.csv\")  # For development, assume test has responses\n",
    "\n",
    "# 2. Combine Datasets and Split (80% Train, 20% Test)\n",
    "combined_prompts = pd.concat([train_prompts, dev_prompts], ignore_index=True)\n",
    "combined_responses = pd.concat([train_responses, dev_responses], ignore_index=True)\n",
    "train_prompts, test_prompts, train_responses, test_responses = train_test_split(\n",
    "    combined_prompts, combined_responses, test_size=0.2, random_state=100\n",
    ")\n",
    "\n",
    "# 3. Preprocessing with Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens for improved normalization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Process the prompts with the updated preprocessing\n",
    "train_prompts[\"tokens\"] = train_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "test_prompts[\"tokens\"] = test_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# 4. Vectorization & Retrieval using Google News Word2Vec\n",
    "# Load spaCy's medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Google News Word2Vec model via gensim\n",
    "google_news_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_spacy_vector(tokens):\n",
    "    # Convert list of tokens to string and get spaCy's document vector\n",
    "    return nlp(\" \".join(tokens)).vector\n",
    "\n",
    "def get_google_news_vector(tokens, vector_size=300):\n",
    "    # Average the Google News Word2Vec embeddings.\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        # Google News embeddings are case-sensitive. Tried different casings.\n",
    "        if word in google_news_model:\n",
    "            vectors.append(google_news_model[word])\n",
    "        elif word.upper() in google_news_model:\n",
    "            vectors.append(google_news_model[word.upper()])\n",
    "        elif word.capitalize() in google_news_model:\n",
    "            vectors.append(google_news_model[word.capitalize()])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Precompute vectors for the training set using both methods\n",
    "train_prompts[\"spacy_vector\"] = train_prompts[\"tokens\"].apply(get_spacy_vector)\n",
    "train_prompts[\"google_news_vector\"] = train_prompts[\"tokens\"].apply(lambda tokens: get_google_news_vector(tokens))\n",
    "\n",
    "# Convert training vectors to matrices for similarity computation\n",
    "train_spacy_matrix = np.vstack(train_prompts[\"spacy_vector\"].values)\n",
    "train_google_news_matrix = np.vstack(train_prompts[\"google_news_vector\"].values)\n",
    "\n",
    "# Precompute test vectors similarly\n",
    "test_prompts[\"spacy_vector\"] = test_prompts[\"tokens\"].apply(get_spacy_vector)\n",
    "test_prompts[\"google_news_vector\"] = test_prompts[\"tokens\"].apply(lambda tokens: get_google_news_vector(tokens))\n",
    "\n",
    "# Retrieval using an ensemble of spaCy and Google News Word2Vec (equal weights)\n",
    "retrieved_responses = []\n",
    "w_spacy = 0.5\n",
    "w_google_news = 0.5\n",
    "\n",
    "for idx, row in test_prompts.iterrows():\n",
    "    test_spacy_vec = row[\"spacy_vector\"].reshape(1, -1)\n",
    "    test_google_news_vec = row[\"google_news_vector\"].reshape(1, -1)\n",
    "    \n",
    "    sim_spacy = cosine_similarity(test_spacy_vec, train_spacy_matrix)[0]\n",
    "    sim_google_news = cosine_similarity(test_google_news_vec, train_google_news_matrix)[0]\n",
    "    \n",
    "    combined_sim = w_spacy * sim_spacy + w_google_news * sim_google_news\n",
    "    best_match_idx = np.argmax(combined_sim)\n",
    "    retrieved_response = train_responses.iloc[best_match_idx][\"model_response\"]\n",
    "    retrieved_responses.append(retrieved_response)\n",
    "\n",
    "\n",
    "# 5. Compute BLEU Score on Test Split\n",
    "test_split = test_prompts.copy()\n",
    "test_split[\"retrieved_response\"] = retrieved_responses\n",
    "test_split[\"model_response\"] = test_responses[\"model_response\"].astype(str)\n",
    "test_split[\"retrieved_response\"] = test_split[\"retrieved_response\"].astype(str)\n",
    "\n",
    "smoothing_function = SmoothingFunction()\n",
    "test_split[\"bleu_score\"] = test_split.apply(\n",
    "    lambda x: sentence_bleu(\n",
    "        [x[\"model_response\"].split()],\n",
    "        x[\"retrieved_response\"].split(),\n",
    "        weights=(0.5, 0.5, 0, 0),\n",
    "        smoothing_function=smoothing_function.method3\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Average BLEU on Test Split (Ensemble spaCy + FastText):\", test_split[\"bleu_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the track_2_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV created successfully:\n",
      "                    conversation_id                       response_id\n",
      "0  0cf125095fa74e129f9b7b6054d2993e  3d3e33d6cb114ff990e82cea8c1db716\n",
      "1  e6296e2a7a554a3db3152704d065498e  86ee55deab3e4197b5b2df0f94d9c5ef\n",
      "2  ee22ccf57c064f5f955f1fd2f9ed5e90  1a9bbe337a78466b93b001ff7af8c4c0\n",
      "3  f5ef6be6d11746e39ec404496c307ab8  66bb4159f47c48ebabcd028de3b944a7\n",
      "4  1fcea667861046d1834b17e7851dcca4  26fddc26f1a94412b4c3e6eb79af4ef2\n",
      "Total rows: 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# 1. Loading Data\n",
    "train_prompts = pd.read_csv(\"train_prompts.csv\")\n",
    "train_responses = pd.read_csv(\"train_responses.csv\")\n",
    "dev_prompts = pd.read_csv(\"dev_prompts.csv\")\n",
    "dev_responses = pd.read_csv(\"dev_responses.csv\")\n",
    "test_prompts = pd.read_csv(\"test_prompts.csv\")\n",
    "\n",
    "# Remove duplicates if they exist\n",
    "train_prompts = train_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "dev_prompts = dev_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "test_prompts = test_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "\n",
    "# 2. Combine TRAIN and DEV datasets for training\n",
    "combined_prompts = pd.concat([train_prompts, dev_prompts], ignore_index=True)\n",
    "combined_responses = pd.concat([train_responses, dev_responses], ignore_index=True)\n",
    "\n",
    "# 3. Preprocessing with Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens for improved normalization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load spaCy's medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load Google News Word2Vec model\n",
    "google_news_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_google_news_vector(tokens, vector_size=300):\n",
    "    # Average the Google News Word2Vec embeddings\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        # Google News embeddings are case-sensitive. Tried different casings.\n",
    "        if word in google_news_model:\n",
    "            vectors.append(google_news_model[word])\n",
    "        elif word.upper() in google_news_model:\n",
    "            vectors.append(google_news_model[word.upper()])\n",
    "        elif word.capitalize() in google_news_model:\n",
    "            vectors.append(google_news_model[word.capitalize()])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Preprocess prompts\n",
    "combined_prompts[\"tokens\"] = combined_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "test_prompts[\"tokens\"] = test_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Compute vectors for combined prompts\n",
    "combined_prompts[\"google_news_vector\"] = combined_prompts[\"tokens\"].apply(lambda tokens: get_google_news_vector(tokens))\n",
    "test_prompts[\"google_news_vector\"] = test_prompts[\"tokens\"].apply(lambda tokens: get_google_news_vector(tokens))\n",
    "\n",
    "# Convert training vectors to matrix\n",
    "combined_matrix = np.vstack(combined_prompts[\"google_news_vector\"].values)\n",
    "test_matrix = np.vstack(test_prompts[\"google_news_vector\"].values)\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity(test_matrix, combined_matrix)\n",
    "retrieved_indices = np.argmax(similarities, axis=1)\n",
    "\n",
    "# Create submission CSV\n",
    "submission = pd.DataFrame({\n",
    "    'conversation_id': test_prompts['conversation_id'].reset_index(drop=True),\n",
    "    'response_id': combined_prompts.iloc[retrieved_indices]['conversation_id'].reset_index(drop=True)\n",
    "})\n",
    "\n",
    "# Save submission CSV\n",
    "submission.to_csv('track_2_test.csv', index=False)\n",
    "\n",
    "print(\"Submission CSV created successfully:\")\n",
    "print(submission.head())\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
