{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 3\n",
    "**(✨BONUS✨): Open Text Representation**. In this track, you can use any combination of the two previous or another representation method. This could include methods not covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these versions of the libraries in case of conflicts between them\n",
    "# Terminal\n",
    "# pip install nltk==3.9.1 pandas==2.2.3 scikit-learn==1.6.1 numpy==1.26.4 sentence-transformers==3.4.1\n",
    "# Jupyter Notebook\n",
    "# %pip install nltk==3.9.1 pandas==2.2.3 scikit-learn==1.6.1 numpy==1.26.4 sentence-transformers==3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucamilani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucamilani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/lucamilani/opt/anaconda3/envs/nlp_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU on Test Split (Two-Phase TF-IDF + SBERT): 0.10239148667597395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. Loading Data\n",
    "train_prompts = pd.read_csv(\"train_prompts.csv\")\n",
    "train_responses = pd.read_csv(\"train_responses.csv\")\n",
    "dev_prompts   = pd.read_csv(\"dev_prompts.csv\")\n",
    "dev_responses = pd.read_csv(\"dev_responses.csv\")\n",
    "test_prompts  = pd.read_csv(\"test_prompts.csv\")  # For development, assume test has responses\n",
    "\n",
    "# 2. Combine Datasets and Split (80% Train, 20% Test)\n",
    "combined_prompts   = pd.concat([train_prompts, dev_prompts], ignore_index=True)\n",
    "combined_responses = pd.concat([train_responses, dev_responses], ignore_index=True)\n",
    "train_prompts, test_prompts, train_responses, test_responses = train_test_split(\n",
    "    combined_prompts, combined_responses, test_size=0.2, random_state=100\n",
    ")\n",
    "\n",
    "# 3. Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)  # Return a joined string for TF-IDF\n",
    "\n",
    "train_prompts[\"processed_text\"] = train_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "test_prompts[\"processed_text\"]  = test_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# 4. Vectorization & Two-Phase Retrieval\n",
    "# Phase 1: TF-IDF Retrieval\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_prompts[\"processed_text\"])\n",
    "tfidf_test  = tfidf_vectorizer.transform(test_prompts[\"processed_text\"])\n",
    "\n",
    "# Phase 2: SBERT Re-Ranking\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Precompute SBERT embeddings for training prompts (use original text for full semantic content)\n",
    "train_sbert = np.vstack(train_prompts[\"user_prompt\"].astype(str).apply(lambda x: sbert_model.encode(x)).values)\n",
    "\n",
    "\n",
    "# Retrieve candidate responses:\n",
    "top_N = 100  # number of candidates from TF-IDF phase\n",
    "\n",
    "retrieved_responses = []\n",
    "for i in range(tfidf_test.shape[0]):\n",
    "    # Get TF-IDF similarity scores between the test prompt and all training prompts\n",
    "    tfidf_sim = cosine_similarity(tfidf_test[i], tfidf_train).flatten()\n",
    "    # Select top_N indices based on TF-IDF scores\n",
    "    candidate_indices = np.argsort(tfidf_sim)[-top_N:]\n",
    "    \n",
    "    # Compute SBERT similarity for the test prompt against the candidate set\n",
    "    test_embedding = sbert_model.encode(test_prompts.iloc[i][\"user_prompt\"])\n",
    "    candidate_embeddings = train_sbert[candidate_indices]\n",
    "    sbert_sim = cosine_similarity(test_embedding.reshape(1, -1), candidate_embeddings).flatten()\n",
    "    \n",
    "    # Combine scores: re-rank solely based on SBERT \n",
    "    best_candidate_idx = candidate_indices[np.argmax(sbert_sim)]\n",
    "    \n",
    "    # Retrieve the corresponding response from the training responses\n",
    "    retrieved_response = train_responses.iloc[best_candidate_idx][\"model_response\"]\n",
    "    retrieved_responses.append(retrieved_response)\n",
    "\n",
    "# 5. Compute BLEU Score on Test Split\n",
    "test_split = test_prompts.copy()\n",
    "test_split[\"retrieved_response\"] = retrieved_responses\n",
    "test_split[\"model_response\"] = test_responses[\"model_response\"].astype(str)\n",
    "test_split[\"retrieved_response\"] = test_split[\"retrieved_response\"].astype(str)\n",
    "\n",
    "smoothing_function = SmoothingFunction()\n",
    "test_split[\"bleu_score\"] = test_split.apply(\n",
    "    lambda x: sentence_bleu(\n",
    "        [x[\"model_response\"].split()],\n",
    "        x[\"retrieved_response\"].split(),\n",
    "        weights=(0.5, 0.5, 0, 0),\n",
    "        smoothing_function=smoothing_function.method3\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Average BLEU on Test Split (Two-Phase TF-IDF + SBERT):\", test_split[\"bleu_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the track_3_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamilani/opt/anaconda3/envs/nlp_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV created successfully:\n",
      "                    conversation_id                       response_id\n",
      "0  0cf125095fa74e129f9b7b6054d2993e  a0addd7b3ccd4de4a7e65ca4ecc853cb\n",
      "1  e6296e2a7a554a3db3152704d065498e  36c96269917546819714296935de4793\n",
      "2  ee22ccf57c064f5f955f1fd2f9ed5e90  80e76b8afd034f2bbfe5da8c80eab817\n",
      "3  f5ef6be6d11746e39ec404496c307ab8  66bb4159f47c48ebabcd028de3b944a7\n",
      "4  1fcea667861046d1834b17e7851dcca4  717f4d21de9c4ff6863049d546fc310e\n",
      "Total rows: 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# 1. Loading Data\n",
    "train_prompts = pd.read_csv(\"train_prompts.csv\")\n",
    "train_responses = pd.read_csv(\"train_responses.csv\")\n",
    "dev_prompts = pd.read_csv(\"dev_prompts.csv\")\n",
    "dev_responses = pd.read_csv(\"dev_responses.csv\")\n",
    "test_prompts = pd.read_csv(\"test_prompts.csv\")\n",
    "\n",
    "# Remove duplicates if they exist\n",
    "train_prompts = train_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "dev_prompts = dev_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "test_prompts = test_prompts.drop_duplicates(subset=['conversation_id'])\n",
    "\n",
    "# 2. Combine TRAIN and DEV datasets for training\n",
    "combined_prompts = pd.concat([train_prompts, dev_prompts], ignore_index=True)\n",
    "combined_responses = pd.concat([train_responses, dev_responses], ignore_index=True)\n",
    "\n",
    "# 3. Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)  # Return a joined string for TF-IDF\n",
    "\n",
    "# Preprocess prompts\n",
    "combined_prompts[\"processed_text\"] = combined_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "test_prompts[\"processed_text\"] = test_prompts[\"user_prompt\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# 4. Vectorization & Two-Phase Retrieval\n",
    "# Phase 1: TF-IDF Retrieval\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_combined = tfidf_vectorizer.fit_transform(combined_prompts[\"processed_text\"])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_prompts[\"processed_text\"])\n",
    "\n",
    "# Phase 2: SBERT Re-Ranking\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Precompute SBERT embeddings for combined prompts (use original text for full semantic content)\n",
    "combined_sbert = np.vstack(combined_prompts[\"user_prompt\"].astype(str).apply(lambda x: sbert_model.encode(x)).values)\n",
    "\n",
    "# Retrieve candidate responses\n",
    "top_N = 100  # number of candidates from TF-IDF phase\n",
    "retrieved_indices = []\n",
    "\n",
    "for i in range(tfidf_test.shape[0]):\n",
    "    # Get TF-IDF similarity scores between the test prompt and all combined prompts\n",
    "    tfidf_sim = cosine_similarity(tfidf_test[i], tfidf_combined).flatten()\n",
    "    # Select top_N indices based on TF-IDF scores\n",
    "    candidate_indices = np.argsort(tfidf_sim)[-top_N:]\n",
    "    \n",
    "    # Compute SBERT similarity for the test prompt against the candidate set\n",
    "    test_embedding = sbert_model.encode(test_prompts.iloc[i][\"user_prompt\"])\n",
    "    candidate_embeddings = combined_sbert[candidate_indices]\n",
    "    sbert_sim = cosine_similarity(test_embedding.reshape(1, -1), candidate_embeddings).flatten()\n",
    "    \n",
    "    # Select the best candidate index\n",
    "    best_candidate_idx = candidate_indices[np.argmax(sbert_sim)]\n",
    "    retrieved_indices.append(best_candidate_idx)\n",
    "\n",
    "# Create submission CSV\n",
    "submission = pd.DataFrame({\n",
    "    'conversation_id': test_prompts['conversation_id'].reset_index(drop=True),\n",
    "    'response_id': combined_prompts.iloc[retrieved_indices]['conversation_id'].reset_index(drop=True)\n",
    "})\n",
    "\n",
    "# Save submission CSV\n",
    "submission.to_csv('track_3_test.csv', index=False)\n",
    "\n",
    "print(\"Submission CSV created successfully:\")\n",
    "print(submission.head())\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
